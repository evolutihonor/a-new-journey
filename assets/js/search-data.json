{
  
    
        "post0": {
            "title": "First_post",
            "content": "Hello World . This is my first post. .",
            "url": "https://evolutihonor.github.io/a-new-journey/2020/09/23/First_Post.html",
            "relUrl": "/2020/09/23/First_Post.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Decision Tree Regressor and LGBM Regressor",
            "content": "import numpy as np import pandas as pd from sklearn import linear_model from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sns import random import lightgbm as lgb from sklearn.model_selection import cross_val_score, cross_val_predict from sklearn.metrics import mean_squared_log_error from sklearn import metrics from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import r2_score from sklearn.model_selection import KFold, ShuffleSplit, RepeatedKFold, TimeSeriesSplit, GridSearchCV, RandomizedSearchCV import warnings warnings.filterwarnings(&quot;ignore&quot;) . def log_transform(s, reverse=False): if reverse: return np.exp(s)-1 else: return np.log(s+1) def ansc_transform(s, reverse=False): if reverse: return ((s/2)**2)-0.125 else: return np.sqrt(s+0.375)*2 def no_transform(s, reverse=False): return s def frmn_transform(s, reverse=False): if reverse: return (s/2)**2 else: return np.sqrt(s+1)+np.sqrt(s) transforms = {&#39;none&#39;:no_transform, &#39;log&#39;: log_transform, &#39;anscombe&#39;: ansc_transform, &#39;freeman&#39;: frmn_transform} def transform_target(df, method): target = &#39;target_&#39;+method df[target] = transforms[method](df[&#39;cnt&#39;]) return df, target . def add_commute(df, features, ordinal=False): if not ordinal: df[&#39;commute&#39;] = (df[&#39;is_weekend&#39;]==0) &amp; df[&#39;timeofday&#39;].isin([7,8,9,17,18]) features.append(&#39;commute&#39;) else: df[&#39;commute&#39;] = 0 df.loc[((df[&#39;is_weekend&#39;]==0) &amp; df[&#39;timeofday&#39;].isin([7,9,16,19])),&#39;commute&#39;] = 1 df.loc[((df[&#39;is_weekend&#39;]==0) &amp; df[&#39;timeofday&#39;].isin([17,18])),&#39;commute&#39;] = 2 df.loc[((df[&#39;is_weekend&#39;]==0) &amp; df[&#39;timeofday&#39;].isin([8])),&#39;commute&#39;] = 3 features.append(&#39;commute&#39;) return df, features def add_sleeptime(df, features): df[&#39;sleeptime&#39;] = (df[&#39;timeofday&#39;].isin([0,1,2,3,4,5,23])) features.append(&#39;sleeptime&#39;) return df, features def process_temps(df, features): df[&#39;tm&#39;] = (df[&#39;t1&#39;]+df[&#39;t2&#39;])/2 features.append(&#39;tm&#39;) return df, features def encode_timeofday(source, dataframes, features): for df in dataframes: df[&#39;tmc_tod&#39;] = 0 for tod in range(24): val = source.loc[source[&#39;timeofday&#39;]==tod, &#39;cnt&#39;].mean() for df in dataframes: df.loc[df[&#39;timeofday&#39;]==tod, &#39;tmc_tod&#39;] = val features.remove(&#39;timeofday&#39;) features.append(&#39;tmc_tod&#39;) return dataframes, features def eliminate_outliers(df): df[&#39;outlier&#39;] = 0 df.loc[df[&#39;cnt&#39;]&gt;5600,&#39;outlier&#39;] = 1 df.iloc[8382:8720].outlier = 1 df = df[df[&#39;outlier&#39;]==0] df.reset_index(drop=True,inplace=True) return df def split_oot(df, ratio): L = len(df) split_index = int(L*ratio) train = df.iloc[:split_index] test = df.iloc[split_index:] return train, test def split_oos(df, ratio): return train_test_split(df, test_size=(1-ratio), random_state=42) def split_data(df, split=&quot;oot&quot;, ratio=0.7): splits = {&#39;oot&#39;:split_oot, &#39;oos&#39;:split_oos} train, test = splits[split](df, ratio) return train, test . def calc_results(actual, pred, experiment_name, n_sample=0, n_features=0): result = {} result[&#39;experiment&#39;] = experiment_name result[&#39;MSE&#39;] = mean_squared_error(actual, pred) result[&#39;MAE&#39;] = mean_absolute_error(actual, pred) result[&#39;RMSE&#39;] = (mean_squared_error(actual, pred)**0.5) result[&#39;R2&#39;] = r2_score(actual, pred) #result[&#39;R2Adj&#39;] = 1-(1-result[&#39;R2&#39;])*(n_sample-1)/(n_sample-n_features-1) #result[&#39;ExpVar&#39;] = explained_variance_score(actual, pred) return result . def preprocess(df, encode_weather=True, encode_season=True, commute=True, sleep=True, elim_outliers=False, target_mean_enc=True): features = [&#39;hum&#39;, &#39;t1&#39;, &#39;weather_code&#39;, &#39;wind_speed&#39;, &quot;season&quot;, &#39;is_holiday&#39;, &#39;is_weekend&#39;] df[&#39;timeofday&#39;] = pd.to_datetime(df[&#39;timestamp&#39;]).dt.hour features.append(&#39;timeofday&#39;) if commute: df, features = add_commute(df, features, True) if sleep: df, features = add_sleeptime(df, features) if encode_weather: df[&#39;wc_1&#39;] = df.weather_code==1 df[&#39;wc_2&#39;] = df.weather_code==2 df[&#39;wc_3&#39;] = df.weather_code==3 df[&#39;wc_4&#39;] = df.weather_code==4 df[&#39;wc_7&#39;] = df.weather_code==7 df[&#39;wc_10&#39;] = df.weather_code==10 #df[&#39;wc_26&#39;] = df.weather_code==26 features.remove(&#39;weather_code&#39;) features.extend([&#39;wc_1&#39;, &#39;wc_2&#39;, &#39;wc_3&#39;, &#39;wc_4&#39;, &#39;wc_7&#39;, &#39;wc_10&#39;]) if encode_season: df[&#39;s_0&#39;] = df.season==0 df[&#39;s_1&#39;] = df.season==1 df[&#39;s_2&#39;] = df.season==2 #df[&#39;s_3&#39;] = df.season==3 features.remove(&#39;season&#39;) features.extend([&#39;s_0&#39;,&#39;s_1&#39;, &#39;s_2&#39;]) if elim_outliers: df = eliminate_outliers(df) return df, features . def fit_model(X_train, Y_train, params=None): if params is None: regressor = DecisionTreeRegressor() else: regressor = DecisionTreeRegressor(**params) regressor.fit(X_train, Y_train) return regressor def evaluate_model(label, regressor, X_test, Y_test, transform, print_out=True): pred = regressor.predict(X_test) pred_raw = transforms[transform](pred, reverse=True) actual_raw = transforms[transform](Y_test, reverse=True) results = calc_results(actual_raw, pred_raw, label) if print_out: print(results) return results . OOS vs OOT split . transform = &#39;none&#39; df = pd.read_csv(&#39;bikes_london.csv&#39;) df, target = transform_target(df, transform) df, features = preprocess(df) workset, holdoutset = split_data(df, &#39;oot&#39;, 0.9) . train, test = split_data(workset,&#39;oos&#39;,0.75) oos_model = fit_model(train[features], train[target]) evaluate_model(&#39;OOS validation&#39;, oos_model, test[features], test[target], transform) print() . {&#39;experiment&#39;: &#39;OOS validation&#39;, &#39;MSE&#39;: 122132.89982133741, &#39;MAE&#39;: 201.19321082184788, &#39;RMSE&#39;: 349.4751776898288, &#39;R2&#39;: 0.9030118156629421} . train, test = split_data(workset,&#39;oot&#39;,0.75) oot_model = fit_model(train[features], train[target]) evaluate_model(&#39;OOT validation&#39;, oot_model, test[features], test[target], transform) print() . {&#39;experiment&#39;: &#39;OOT validation&#39;, &#39;MSE&#39;: 204799.5246937213, &#39;MAE&#39;: 262.09047983665135, &#39;RMSE&#39;: 452.5478148148782, &#39;R2&#39;: 0.8655804515586282} . evaluate_model(&#39;OOS holdout scores&#39;, oos_model, holdoutset[features], holdoutset[target], transform) evaluate_model(&#39;OOT holdout scores&#39;, oot_model, holdoutset[features], holdoutset[target], transform) print() . {&#39;experiment&#39;: &#39;OOS holdout scores&#39;, &#39;MSE&#39;: 192317.17695177958, &#39;MAE&#39;: 243.07835820895522, &#39;RMSE&#39;: 438.539823678283, &#39;R2&#39;: 0.7771839462226688} {&#39;experiment&#39;: &#39;OOT holdout scores&#39;, &#39;MSE&#39;: 161108.95235361654, &#39;MAE&#39;: 239.3955223880597, &#39;RMSE&#39;: 401.38379682495474, &#39;R2&#39;: 0.8133413688750555} . . Cross validation . def Apply_CrossValidation(fold, dataset, features, target, metric=None, params=None): results = pd.DataFrame(columns=[&#39;experiment&#39;,&#39;MSE&#39;,&#39;MAE&#39;,&#39;RMSE&#39;,&#39;R2&#39;]) i = 0 for i_train, i_test in fold.split(dataset): X_train, Y_train = workset.iloc[i_train][features], workset.iloc[i_train][target] X_test, Y_test = workset.iloc[i_test][features], workset.iloc[i_test][target] fold_model = fit_model(X_train, Y_train, params) result = evaluate_model(&#39;Fold &#39;+str(i), fold_model, X_test, Y_test, transform, False) results = results.append(result, ignore_index=True) i+=1 if metric is not None: print(metric + &quot; mean : &quot;, results[metric].mean()) print(metric + &quot; std : &quot;, results[metric].std()) return results . transform = &#39;none&#39; df = pd.read_csv(&#39;bikes_london.csv&#39;) df, target = transform_target(df, transform) df, features = preprocess(df) workset, holdoutset = split_data(df, &#39;oot&#39;, 0.90) . kf = KFold(n_splits=4, shuffle=True) results = Apply_CrossValidation(kf, workset, features, target, &#39;RMSE&#39;) . RMSE mean : 339.1686759126851 RMSE std : 15.228270265681376 . kf = KFold(n_splits=4, shuffle=False) results = Apply_CrossValidation(kf, workset, features, target, &#39;RMSE&#39;) . RMSE mean : 391.52263400203833 RMSE std : 55.018695021114816 . rkf = RepeatedKFold(n_splits=4, n_repeats=5) results = Apply_CrossValidation(rkf, workset, features, target, &#39;RMSE&#39;) . RMSE mean : 348.49094054183195 RMSE std : 9.731589539037031 . sf = ShuffleSplit(n_splits=4) results = Apply_CrossValidation(sf, workset, features, target, &#39;RMSE&#39;) . RMSE mean : 347.25990959582623 RMSE std : 37.70945401922651 . tsf = TimeSeriesSplit(max_train_size=None, n_splits=4) results = Apply_CrossValidation(tsf, workset, features, target, &#39;RMSE&#39;) . RMSE mean : 411.6945545226182 RMSE std : 41.048045211194896 . . tsf = TimeSeriesSplit(max_train_size=None, n_splits=4) params = {&#39;min_samples_leaf&#39;:6} results = Apply_CrossValidation(tsf, workset, features, target, &#39;RMSE&#39;, params) . RMSE mean : 373.04282719988794 RMSE std : 73.66461877698802 . param_grid = { &quot;min_samples_split&quot;: [2, 3, 4, 5, 6, 7, 8, 12, 20, 30], &quot;max_depth&quot;: [6, 8, 9, 10, 11, 12, 15, 18, 27, 59], &quot;min_samples_leaf&quot;: [2,3,4,5,6,7,8,9,10,25,50], &quot;max_leaf_nodes&quot;: [127, 255, 511, 1023, 4095, 8191, 16383] } . random_search = RandomizedSearchCV( DecisionTreeRegressor(), param_grid, n_iter=200, scoring=&#39;neg_root_mean_squared_error&#39;, cv=TimeSeriesSplit(max_train_size=None, n_splits=4), verbose=1) # YOUR CODE HERE random_search.fit(workset[features], workset[target]) . Fitting 4 folds for each of 200 candidates, totalling 800 fits . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 800 out of 800 | elapsed: 20.0s finished . RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=4), estimator=DecisionTreeRegressor(), n_iter=200, param_distributions={&#39;max_depth&#39;: [6, 8, 9, 10, 11, 12, 15, 18, 27, 59], &#39;max_leaf_nodes&#39;: [127, 255, 511, 1023, 4095, 8191, 16383], &#39;min_samples_leaf&#39;: [2, 3, 4, 5, 6, 7, 8, 9, 10, 25, 50], &#39;min_samples_split&#39;: [2, 3, 4, 5, 6, 7, 8, 12, 20, 30]}, scoring=&#39;neg_root_mean_squared_error&#39;, verbose=1) . print(&#39;Best Score:&#39;, random_search.best_score_) print(&#39;Best parameter values {}&#39;.format(random_search.best_params_)) . Best Score: -355.50947171052 Best parameter values {&#39;min_samples_split&#39;: 8, &#39;min_samples_leaf&#39;: 4, &#39;max_leaf_nodes&#39;: 255, &#39;max_depth&#39;: 59} . Final training with best parameters . train, test = split_data(workset,&#39;oot&#39;,0.75) oot_model = fit_model(train[features], train[target],random_search.best_params_) evaluate_model(&#39;OOT validation&#39;, oot_model, test[features], test[target], transform) evaluate_model(&#39;OOT holdout scores&#39;, oot_model, holdoutset[features], holdoutset[target], transform) print() . {&#39;experiment&#39;: &#39;OOT validation&#39;, &#39;MSE&#39;: 129490.73138047603, &#39;MAE&#39;: 227.47905159159802, &#39;RMSE&#39;: 359.84820602648006, &#39;R2&#39;: 0.9150091502139106} {&#39;experiment&#39;: &#39;OOT holdout scores&#39;, &#39;MSE&#39;: 125398.08871964493, &#39;MAE&#39;: 206.41119712389175, &#39;RMSE&#39;: 354.11592553801495, &#39;R2&#39;: 0.8547154875992349} . print(&quot;node count : &quot;, oot_model.tree_.node_count) print(&quot;depth : &quot;, oot_model.tree_.max_depth) . node count : 509 depth : 14 . imp = oot_model.feature_importances_ plt.figure(figsize=(8,6)) ax = sns.barplot(x=imp, y=features) plt.show() . plt.figure(figsize=(8,8)) pred = oot_model.predict(holdoutset[features]) pred_raw = transforms[transform](pred, reverse=True) actual_raw = transforms[transform](holdoutset[target], reverse=True) plt.scatter(pred_raw, actual_raw, alpha=0.1) Mx = max([max(pred_raw),max(actual_raw)]) plt.plot([0,Mx],[0,Mx],&quot;r&quot;) plt.show() . rng = 24*7 I = random.randint(0,len(holdoutset)-rng) # a random week plt.figure(figsize=(15,5)) plt.plot(actual_raw.values[I:I+rng]) plt.plot(pred_raw[I:I+rng]) plt.show() . plt.figure(figsize=(15,5)) residual = actual_raw-pred_raw plt.scatter(actual_raw, residual, alpha=0.2) plt.show() . Part 4 LightGBM Gradient boosted decision tree regressor . def fit_booster(X_train, Y_train, params=None): if params is None: regressor = lgb.LGBMRegressor() else: regressor = lgb.LGBMRegressor(**params) regressor.fit(X_train, Y_train) return regressor def evaluate_booster(label, regressor, X_test, Y_test, transform, print_out=True): pred = regressor.predict(X_test) pred_raw = transforms[transform](pred, reverse=True) actual_raw = transforms[transform](Y_test, reverse=True) results = calc_results(actual_raw, pred_raw, label) if print_out: print(results) return results . train, test = split_data(workset,&#39;oos&#39;,0.75) oos_model = fit_booster(train[features], train[target]) evaluate_booster(&#39;OOS validation&#39;, oos_model, test[features], test[target], transform) print() . {&#39;experiment&#39;: &#39;OOS validation&#39;, &#39;MSE&#39;: 63418.321064090494, &#39;MAE&#39;: 144.29053708340098, &#39;RMSE&#39;: 251.8299447327313, &#39;R2&#39;: 0.9496382397969058} . train, test = split_data(workset,&#39;oot&#39;,0.75) oot_model = fit_booster(train[features], train[target]) evaluate_booster(&#39;OOT validation&#39;, oot_model, test[features], test[target], transform) print() . {&#39;experiment&#39;: &#39;OOT validation&#39;, &#39;MSE&#39;: 87299.66923913675, &#39;MAE&#39;: 191.25188276101233, &#39;RMSE&#39;: 295.46517432539616, &#39;R2&#39;: 0.9427011262074201} . evaluate_booster(&#39;OOS holdout scores&#39;, oos_model, holdoutset[features], holdoutset[target], transform) evaluate_booster(&#39;OOT holdout scores&#39;, oot_model, holdoutset[features], holdoutset[target], transform) print() . {&#39;experiment&#39;: &#39;OOS holdout scores&#39;, &#39;MSE&#39;: 104913.8939548961, &#39;MAE&#39;: 182.68336339428197, &#39;RMSE&#39;: 323.90414315796596, &#39;R2&#39;: 0.8784481957984198} {&#39;experiment&#39;: &#39;OOT holdout scores&#39;, &#39;MSE&#39;: 105565.70237465709, &#39;MAE&#39;: 183.5380392035854, &#39;RMSE&#39;: 324.9087600768208, &#39;R2&#39;: 0.8776930194682973} . . def LGBM_CrossValidation(fold, dataset, features, target, metric=None, params=None): results = pd.DataFrame(columns=[&#39;experiment&#39;,&#39;MSE&#39;,&#39;MAE&#39;,&#39;RMSE&#39;,&#39;R2&#39;]) i = 0 for i_train, i_test in fold.split(dataset): X_train, Y_train = workset.iloc[i_train][features], workset.iloc[i_train][target] X_test, Y_test = workset.iloc[i_test][features], workset.iloc[i_test][target] fold_model = fit_booster(X_train, Y_train, params) result = evaluate_model(&#39;Fold &#39;+str(i), fold_model, X_test, Y_test, transform, False) results = results.append(result, ignore_index=True) i+=1 if metric is not None: print(metric + &quot; mean : &quot;, results[metric].mean()) print(metric + &quot; std : &quot;, results[metric].std()) return results . kf = KFold(n_splits=4, shuffle=True) results = LGBM_CrossValidation(kf, workset, features, target, &#39;RMSE&#39;) . RMSE mean : 241.9500072606859 RMSE std : 7.81130797742133 . tsf = TimeSeriesSplit(max_train_size=None, n_splits=4) results = LGBM_CrossValidation(tsf, workset, features, target, &#39;RMSE&#39;) . RMSE mean : 314.2409528238817 RMSE std : 71.84917505433991 . param_grid = {&#39;boosting_type&#39;:[&quot;gbdt&quot;], &#39;objective&#39;:[&#39;regression&#39;], &#39;n_estimators&#39;:[500], &#39;num_leaves&#39;:[3,7,13,17,23,37,51,77,103], &#39;max_depth&#39;:[2,3,5,8,9,10,12,15,21,33], &#39;feature_fraction&#39;:[0.25,0.5,0.75], &#39;bagging_fraction&#39;:[0.25,0.5,0.75], &#39;bagging_freq&#39;:[2,5,8,15], &#39;learning_rate&#39;:[0.01,0.024,0.05] } . random_search = RandomizedSearchCV( lgb.LGBMRegressor(), param_grid, n_iter=200, scoring=&#39;neg_root_mean_squared_error&#39;, cv=TimeSeriesSplit(max_train_size=None, n_splits=4), verbose=1, n_jobs=4) random_search.fit(workset[features], workset[target]) . Fitting 4 folds for each of 200 candidates, totalling 800 fits . [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 42 tasks | elapsed: 6.7s [Parallel(n_jobs=4)]: Done 192 tasks | elapsed: 29.1s [Parallel(n_jobs=4)]: Done 442 tasks | elapsed: 1.0min [Parallel(n_jobs=4)]: Done 792 tasks | elapsed: 1.9min [Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed: 1.9min finished . RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=4), estimator=LGBMRegressor(), n_iter=200, n_jobs=4, param_distributions={&#39;bagging_fraction&#39;: [0.25, 0.5, 0.75], &#39;bagging_freq&#39;: [2, 5, 8, 15], &#39;boosting_type&#39;: [&#39;gbdt&#39;], &#39;feature_fraction&#39;: [0.25, 0.5, 0.75], &#39;learning_rate&#39;: [0.01, 0.024, 0.05], &#39;max_depth&#39;: [2, 3, 5, 8, 9, 10, 12, 15, 21, 33], &#39;n_estimators&#39;: [500], &#39;num_leaves&#39;: [3, 7, 13, 17, 23, 37, 51, 77, 103], &#39;objective&#39;: [&#39;regression&#39;]}, scoring=&#39;neg_root_mean_squared_error&#39;, verbose=1) . print(&#39;Best Score:&#39;, random_search.best_score_) print(&#39;Best parameter values {}&#39;.format(random_search.best_params_)) . Best Score: -310.0396310285107 Best parameter values {&#39;objective&#39;: &#39;regression&#39;, &#39;num_leaves&#39;: 23, &#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 15, &#39;learning_rate&#39;: 0.024, &#39;feature_fraction&#39;: 0.75, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;bagging_freq&#39;: 2, &#39;bagging_fraction&#39;: 0.75} . train, test = split_data(workset,&#39;oot&#39;,0.75) oot_model = fit_booster(train[features], train[target],random_search.best_params_) evaluate_booster(&#39;OOT validation&#39;, oot_model, test[features], test[target], transform) evaluate_booster(&#39;OOT holdout scores&#39;, oot_model, holdoutset[features], holdoutset[target], transform) print() . {&#39;experiment&#39;: &#39;OOT validation&#39;, &#39;MSE&#39;: 81857.83900256577, &#39;MAE&#39;: 188.80945993030608, &#39;RMSE&#39;: 286.1080897188435, &#39;R2&#39;: 0.9462728550197229} {&#39;experiment&#39;: &#39;OOT holdout scores&#39;, &#39;MSE&#39;: 106750.04758310426, &#39;MAE&#39;: 186.84866827011288, &#39;RMSE&#39;: 326.72625787209734, &#39;R2&#39;: 0.8763208532903253} .",
            "url": "https://evolutihonor.github.io/a-new-journey/2020/09/22/Tree-Based-Regressors.html",
            "relUrl": "/2020/09/22/Tree-Based-Regressors.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://evolutihonor.github.io/a-new-journey/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://evolutihonor.github.io/a-new-journey/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://evolutihonor.github.io/a-new-journey/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://evolutihonor.github.io/a-new-journey/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}